{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER with BiLSTM for detecting toxic words spans in text.",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yr53XvnCj-t"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(seed=2021)\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import GRU, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
        "from keras.utils import plot_model\n",
        "from keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nstLV3d-gWeb"
      },
      "source": [
        "### Loading the proccesed data to feed the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD8QLRLZhQ_T"
      },
      "source": [
        "Tokens from each text are taged according to the BIO format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJEfmLlCCrVe"
      },
      "source": [
        "from ast import literal_eval\n",
        "TRAIN_FILE=\"train.csv\"\n",
        "VAL_FILE=\"val.csv\"\n",
        "TEST_FILE=\"test.csv\"\n",
        "\n",
        "\n",
        "train_df=pd.read_csv(TRAIN_FILE)\n",
        "train_df.spans=train_df.spans.apply(lambda row:literal_eval(row))\n",
        "train_df.labels=train_df.labels.apply(lambda row:literal_eval(row))\n",
        "train_df.offset_mapping=train_df.offset_mapping.apply(lambda row:literal_eval(row))\n",
        "train_df.tokens=train_df.tokens.apply(lambda row:literal_eval(row))\n",
        "train_df.BIO_tags=train_df.BIO_tags.apply(lambda row:literal_eval(row))\n",
        "\n",
        "val_df=pd.read_csv(VAL_FILE)\n",
        "val_df.spans=val_df.spans.apply(lambda row:literal_eval(row))\n",
        "val_df.labels=val_df.labels.apply(lambda row:literal_eval(row))\n",
        "val_df.offset_mapping=val_df.offset_mapping.apply(lambda row:literal_eval(row))\n",
        "val_df.tokens=val_df.tokens.apply(lambda row:literal_eval(row))\n",
        "val_df.BIO_tags=val_df.BIO_tags.apply(lambda row:literal_eval(row))\n",
        "\n",
        "test_df=pd.read_csv(TEST_FILE)\n",
        "test_df.spans=test_df.spans.apply(lambda row:literal_eval(row))\n",
        "test_df.labels=test_df.labels.apply(lambda row:literal_eval(row))\n",
        "test_df.offset_mapping=test_df.offset_mapping.apply(lambda row:literal_eval(row))\n",
        "test_df.tokens=test_df.tokens.apply(lambda row:literal_eval(row))\n",
        "test_df.BIO_tags=test_df.BIO_tags.apply(lambda row:literal_eval(row))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2K-HVzpgeNG"
      },
      "source": [
        "### Implementing suggested f1 metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGZ4qRSYDpVB"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1 if len(predictions)==0 else 0\n",
        "    nom = 2*len(set(predictions).intersection(set(gold)))\n",
        "    denom = len(set(predictions))+len(set(gold))\n",
        "    return nom/denom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0JkKhGXgrOX"
      },
      "source": [
        "### Implementing baseline BiLSTM for token based classification for toxic span detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOBGocLwqWfm"
      },
      "source": [
        "np.random.seed(seed=2021)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import GRU, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras.utils import plot_model\n",
        "from keras.metrics import BinaryAccuracy, Precision, Recall, AUC,Accuracy\n",
        "from collections import Counter\n",
        "\n",
        "class RNNSL:\n",
        "\n",
        "    def __init__(self, maxlen=128, batch_size=32, w_embed_size=200, h_embed_size=200, dropout=0.1, patience=1, plot=True, max_epochs=100):\n",
        "        self.maxlen = maxlen\n",
        "        self.METRICS = [Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
        "        self.w_embed_size = w_embed_size\n",
        "        self.h_embed_size = h_embed_size\n",
        "        self.dropout = dropout\n",
        "        self.patience = patience\n",
        "        self.model = None\n",
        "        self.epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.show_the_model = plot\n",
        "        self.threshold = 0.2\n",
        "        self.word_to_index={}\n",
        "        self.class_weights={}\n",
        "        \n",
        "        # self.toxic_label = 2\n",
        "        # self.not_toxic_label = sel  \n",
        "\n",
        "\n",
        "\n",
        "    def set_up_preprocessing(self, tokenized_texts,labels):\n",
        "        #get unique words of corpus\n",
        "        total_words=[w for s in tokenized_texts for w in s]\n",
        "        self.vocab=list(set(total_words))\n",
        "        self.vocab_size=len(self.vocab)\n",
        "\n",
        "        #unique tags\n",
        "        total_tags=[t for l in labels for t in l ]\n",
        "        # tags=list(set(total_tags))\n",
        "        tags=['B-TOX','I-TOX','O']\n",
        "        self.n_tags=len(tags)\n",
        "\n",
        "        # Dictionary word:index pair\n",
        "        # word is key and its value is corresponding index\n",
        "        self.word_to_index = {w : i + 2 for i, w in enumerate(self.vocab)}\n",
        "        self.word_to_index[\"UNK\"] = 1\n",
        "        self.word_to_index[\"PAD\"] = 0\n",
        "\n",
        "        # Dictionary lable:index pair\n",
        "        # label is key and value is index.\n",
        "        self.tag_to_index = {t : i for i, t in enumerate(tags)}\n",
        "        # self.tag_to_index[\"PAD\"] = 0\n",
        "        \n",
        "        \n",
        "        self.idx2word = {i: w for w, i in self.word_to_index.items()}\n",
        "        self.idx2tag = {i: w for w, i in self.tag_to_index.items()}\n",
        "\n",
        "        class_weights=Counter(total_tags)\n",
        "        for k,v in class_weights.items():\n",
        "          self.class_weights[self.tag_to_index[k]]=v/len(total_tags)\n",
        "          \n",
        "\n",
        "  \n",
        "\n",
        "    def X_to_sequences(self, tokenized_texts):\n",
        "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "        # Convert each sentence from list of Token to list of word_index\n",
        "        # Converting each sentence into list of index from list of tokens\n",
        "        X = [[self.word_to_index.get(w,self.word_to_index[\"UNK\"]) for w in s] for s in tokenized_texts]\n",
        "        # Padding each sentence to have the same lenght\n",
        "        X = pad_sequences(maxlen=self.maxlen, sequences=X, padding=\"post\", value=self.word_to_index[\"PAD\"])\n",
        "        return X\n",
        "\n",
        "    def y_to_sequences(self,labels):\n",
        "        # Convert Tag/Label to tag_index\n",
        "        y = [[self.tag_to_index[l_i] for l_i in l] for l in labels]\n",
        "        # Padding each sentence to have the same lenght\n",
        "        y = pad_sequences(maxlen=self.maxlen, sequences=y, padding=\"post\", value=self.tag_to_index[\"O\"])\n",
        "\n",
        "        # One hot encoded labels\n",
        "        from keras.utils import to_categorical\n",
        "        y = [to_categorical(i, num_classes = self.n_tags) for i in y]\n",
        "        y=np.array(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def build(self):\n",
        "        input = Input(shape=(self.maxlen,))\n",
        "        model = Embedding(input_dim=self.vocab_size+2, output_dim=self.w_embed_size, input_length=self.maxlen, mask_zero=True)(input)  # 50-dim embedding\n",
        "        model = Dropout(self.dropout)(model)\n",
        "        model = Bidirectional(LSTM(units=self.h_embed_size, return_sequences=True, recurrent_dropout=self.dropout))(model)  # variational biLSTM\n",
        "        output = TimeDistributed(Dense(self.n_tags, activation=\"softmax\"))(model)\n",
        "        return Model(input, output)\n",
        "\n",
        "\n",
        "    def fit(self, tokenized_texts, labels, validation_data=None, monitor=\"val_auc\"):\n",
        "        # set up the vocabulary and the related methods\n",
        "        self.set_up_preprocessing(tokenized_texts,labels)\n",
        "        X,y=self.X_to_sequences(tokenized_texts),self.y_to_sequences(labels)\n",
        "        print(\"X shape {},y shape {}\".format(X.shape,y.shape))\n",
        "        # build the model and compile it\n",
        "        self.model = self.build()\n",
        "        if self.show_the_model:\n",
        "            print(self.model.summary())\n",
        "            plot_model(self.model, show_shapes=True, to_file=\"neural_sequence_labeler.model.png\")\n",
        "        self.model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=self.METRICS)\n",
        "        \n",
        "        #mode = \"max\" if monitor == \"val_accuracy\" else \"min\"\n",
        "        early = EarlyStopping(monitor=monitor, \n",
        "                              mode=\"min\" if \"loss\" in monitor else \"max\",\n",
        "                              patience=self.patience,\n",
        "                              verbose=1,\n",
        "                              min_delta=0.0001, \n",
        "                              restore_best_weights=True)\n",
        "        \n",
        "        checkpointer=ModelCheckpoint(filepath = 'model.h5',\n",
        "                       verbose = 0,\n",
        "                       mode = \"min\" if \"loss\" in monitor else \"max\",\n",
        "                       save_best_only = True,\n",
        "                       monitor=monitor)\n",
        "\n",
        "        # reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.1,\n",
        "        #                       patience=1, min_lr=0.0001)\n",
        "\n",
        "        callbacks=[early,checkpointer]\n",
        "\n",
        "        # start training\n",
        "        if validation_data is not None:\n",
        "            assert len(validation_data) == 2\n",
        "            vX,vy=self.X_to_sequences(validation_data[0]),self.y_to_sequences(validation_data[1])\n",
        "\n",
        "            history = self.model.fit(x, y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(vx, vy), verbose=1, callbacks=callbacks)\n",
        "        else:\n",
        "            history = self.model.fit(X, y,\n",
        "                                     batch_size=self.batch_size,\n",
        "                                     epochs=self.epochs,\n",
        "                                     validation_split=0.1,\n",
        "                                     verbose=1,\n",
        "                                     callbacks=callbacks)\n",
        "        \n",
        "        return self.model,pd.DataFrame(history.history)\n",
        "  \n",
        "    def tune_threshold(self,pred_label_proba,len_true_labels,th=0.2):\n",
        "      self.label_preds=[]\n",
        "      for tag_pred in pred_label_proba[:len_true_labels]:\n",
        "        # print(tag_pred)\n",
        "        if (tag_pred[0] >=th or tag_pred[1]>=th):\n",
        "          self.label_preds.append(2)\n",
        "        \n",
        "        else:\n",
        "          self.label_preds.append(1)\n",
        "        \n",
        "      return self.label_preds\n",
        "    \n",
        "    def fine_tuning(self,tokenized_texts,BIO_tags,offset_mappings,spans):\n",
        "      len_data=len(tokenized_texts)\n",
        "      \n",
        "      X_val=self.X_to_sequences(tokenized_texts)\n",
        "      pred_proba=self.model.predict(X_val)\n",
        "      \n",
        "      mean_f1_scores=[]\n",
        "      thresholds=[]\n",
        "      \n",
        "      for th in np.arange(0.15,0.5,0.01):\n",
        "        print(f\"Tuning for threshold {th:.2f} ...\")\n",
        "\n",
        "        label_predictions=[]\n",
        "\n",
        "        for i,p in enumerate(pred_proba):\n",
        "          label_predictions.append(self.tune_threshold(p,len(BIO_tags[i]),th))\n",
        "\n",
        "\n",
        "\n",
        "        span_predictions=[]\n",
        "        f1_scores=[]\n",
        "\n",
        "        for i in range(len_data):\n",
        "          \n",
        "          curr_pred_span=[]\n",
        "         \n",
        "          for j,pred_labl in enumerate(label_predictions[i]):\n",
        "            if(pred_labl==2):\n",
        "              curr_pred_span+=list(np.arange(offset_mappings[i][j][0],offset_mappings[i][j][1]))\n",
        "\n",
        "          f1_scores.append(f1(curr_pred_span,spans[i]))        \n",
        "\n",
        "        mean_f1_scores.append(np.mean(f1_scores))\n",
        "        thresholds.append(th)\n",
        "\n",
        "        print(f\"|__ mean f1 scores {np.mean(f1_scores):.3f}\")  \n",
        "        print(f\"|__ threshold {th:.2f}\")\n",
        "        print(20*'=')\n",
        "      \n",
        "      return mean_f1_scores,thresholds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRM3fQ8Jhjq7"
      },
      "source": [
        "### Instantiating and training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_CppeVoFFsH"
      },
      "source": [
        "clf=RNNSL(maxlen=192,max_epochs=10,patience=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raocd4MdH8nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01032d84-1d1e-4af5-ac31-820ce209c9bf"
      },
      "source": [
        "# num_samples=100\n",
        "model,hist=clf.fit(train_df.iloc[:].tokens.values,train_df.iloc[:].BIO_tags.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape (7939, 192),y shape (7939, 192, 3)\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 192)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 192, 200)          4715200   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 192, 200)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 192, 400)          641600    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 192, 3)            1203      \n",
            "=================================================================\n",
            "Total params: 5,358,003\n",
            "Trainable params: 5,358,003\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "224/224 [==============================] - 482s 2s/step - loss: 0.0900 - precision: 0.9094 - recall: 0.8125 - auc: 0.9562 - val_loss: 0.0486 - val_precision: 0.9476 - val_recall: 0.9419 - val_auc: 0.9791\n",
            "Epoch 2/10\n",
            "224/224 [==============================] - 473s 2s/step - loss: 0.0539 - precision: 0.9368 - recall: 0.9292 - auc: 0.9779 - val_loss: 0.0487 - val_precision: 0.9492 - val_recall: 0.9421 - val_auc: 0.9791\n",
            "Epoch 3/10\n",
            "224/224 [==============================] - 469s 2s/step - loss: 0.0413 - precision: 0.9465 - recall: 0.9364 - auc: 0.9888 - val_loss: 0.0565 - val_precision: 0.9368 - val_recall: 0.9283 - val_auc: 0.9759\n",
            "Epoch 4/10\n",
            "224/224 [==============================] - 465s 2s/step - loss: 0.0339 - precision: 0.9540 - recall: 0.9465 - auc: 0.9930 - val_loss: 0.0570 - val_precision: 0.9404 - val_recall: 0.9336 - val_auc: 0.9752\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX0_KLg3hq3A"
      },
      "source": [
        "### Tuning threshold to determine toxic/non-toxic tokens on validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqFI2vT5LtdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de17e63-4cfa-42ae-affb-81a04c6c54e2"
      },
      "source": [
        "clf.fine_tuning(val_df.tokens,val_df.BIO_tags,val_df.offset_mapping,val_df.spans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tuning for threshold 0.15 ...\n",
            "|__ mean f1 scores 0.592\n",
            "|__ threshold 0.15\n",
            "====================\n",
            "Tuning for threshold 0.16 ...\n",
            "|__ mean f1 scores 0.595\n",
            "|__ threshold 0.16\n",
            "====================\n",
            "Tuning for threshold 0.17 ...\n",
            "|__ mean f1 scores 0.594\n",
            "|__ threshold 0.17\n",
            "====================\n",
            "Tuning for threshold 0.18 ...\n",
            "|__ mean f1 scores 0.593\n",
            "|__ threshold 0.18\n",
            "====================\n",
            "Tuning for threshold 0.19 ...\n",
            "|__ mean f1 scores 0.594\n",
            "|__ threshold 0.19\n",
            "====================\n",
            "Tuning for threshold 0.20 ...\n",
            "|__ mean f1 scores 0.591\n",
            "|__ threshold 0.20\n",
            "====================\n",
            "Tuning for threshold 0.21 ...\n",
            "|__ mean f1 scores 0.591\n",
            "|__ threshold 0.21\n",
            "====================\n",
            "Tuning for threshold 0.22 ...\n",
            "|__ mean f1 scores 0.585\n",
            "|__ threshold 0.22\n",
            "====================\n",
            "Tuning for threshold 0.23 ...\n",
            "|__ mean f1 scores 0.583\n",
            "|__ threshold 0.23\n",
            "====================\n",
            "Tuning for threshold 0.24 ...\n",
            "|__ mean f1 scores 0.583\n",
            "|__ threshold 0.24\n",
            "====================\n",
            "Tuning for threshold 0.25 ...\n",
            "|__ mean f1 scores 0.582\n",
            "|__ threshold 0.25\n",
            "====================\n",
            "Tuning for threshold 0.26 ...\n",
            "|__ mean f1 scores 0.584\n",
            "|__ threshold 0.26\n",
            "====================\n",
            "Tuning for threshold 0.27 ...\n",
            "|__ mean f1 scores 0.581\n",
            "|__ threshold 0.27\n",
            "====================\n",
            "Tuning for threshold 0.28 ...\n",
            "|__ mean f1 scores 0.580\n",
            "|__ threshold 0.28\n",
            "====================\n",
            "Tuning for threshold 0.29 ...\n",
            "|__ mean f1 scores 0.580\n",
            "|__ threshold 0.29\n",
            "====================\n",
            "Tuning for threshold 0.30 ...\n",
            "|__ mean f1 scores 0.578\n",
            "|__ threshold 0.30\n",
            "====================\n",
            "Tuning for threshold 0.31 ...\n",
            "|__ mean f1 scores 0.574\n",
            "|__ threshold 0.31\n",
            "====================\n",
            "Tuning for threshold 0.32 ...\n",
            "|__ mean f1 scores 0.571\n",
            "|__ threshold 0.32\n",
            "====================\n",
            "Tuning for threshold 0.33 ...\n",
            "|__ mean f1 scores 0.565\n",
            "|__ threshold 0.33\n",
            "====================\n",
            "Tuning for threshold 0.34 ...\n",
            "|__ mean f1 scores 0.561\n",
            "|__ threshold 0.34\n",
            "====================\n",
            "Tuning for threshold 0.35 ...\n",
            "|__ mean f1 scores 0.555\n",
            "|__ threshold 0.35\n",
            "====================\n",
            "Tuning for threshold 0.36 ...\n",
            "|__ mean f1 scores 0.551\n",
            "|__ threshold 0.36\n",
            "====================\n",
            "Tuning for threshold 0.37 ...\n",
            "|__ mean f1 scores 0.544\n",
            "|__ threshold 0.37\n",
            "====================\n",
            "Tuning for threshold 0.38 ...\n",
            "|__ mean f1 scores 0.534\n",
            "|__ threshold 0.38\n",
            "====================\n",
            "Tuning for threshold 0.39 ...\n",
            "|__ mean f1 scores 0.534\n",
            "|__ threshold 0.39\n",
            "====================\n",
            "Tuning for threshold 0.40 ...\n",
            "|__ mean f1 scores 0.532\n",
            "|__ threshold 0.40\n",
            "====================\n",
            "Tuning for threshold 0.41 ...\n",
            "|__ mean f1 scores 0.524\n",
            "|__ threshold 0.41\n",
            "====================\n",
            "Tuning for threshold 0.42 ...\n",
            "|__ mean f1 scores 0.523\n",
            "|__ threshold 0.42\n",
            "====================\n",
            "Tuning for threshold 0.43 ...\n",
            "|__ mean f1 scores 0.520\n",
            "|__ threshold 0.43\n",
            "====================\n",
            "Tuning for threshold 0.44 ...\n",
            "|__ mean f1 scores 0.518\n",
            "|__ threshold 0.44\n",
            "====================\n",
            "Tuning for threshold 0.45 ...\n",
            "|__ mean f1 scores 0.511\n",
            "|__ threshold 0.45\n",
            "====================\n",
            "Tuning for threshold 0.46 ...\n",
            "|__ mean f1 scores 0.506\n",
            "|__ threshold 0.46\n",
            "====================\n",
            "Tuning for threshold 0.47 ...\n",
            "|__ mean f1 scores 0.505\n",
            "|__ threshold 0.47\n",
            "====================\n",
            "Tuning for threshold 0.48 ...\n",
            "|__ mean f1 scores 0.492\n",
            "|__ threshold 0.48\n",
            "====================\n",
            "Tuning for threshold 0.49 ...\n",
            "|__ mean f1 scores 0.490\n",
            "|__ threshold 0.49\n",
            "====================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.5920065998413606,\n",
              "  0.5949025041860077,\n",
              "  0.593892304526475,\n",
              "  0.592716169958767,\n",
              "  0.5942245728718716,\n",
              "  0.590915612760518,\n",
              "  0.5907866378478577,\n",
              "  0.5854721245540991,\n",
              "  0.5828515460688904,\n",
              "  0.5834104193578284,\n",
              "  0.5824188571290647,\n",
              "  0.5840018189376568,\n",
              "  0.5809186835032177,\n",
              "  0.579614815532352,\n",
              "  0.5800507114541814,\n",
              "  0.5780872002408833,\n",
              "  0.5739749772650997,\n",
              "  0.5710387536347706,\n",
              "  0.5646187582916177,\n",
              "  0.5614494085984384,\n",
              "  0.5553500085984732,\n",
              "  0.5513080158136425,\n",
              "  0.5439091412138684,\n",
              "  0.5343662708448437,\n",
              "  0.5338516807650362,\n",
              "  0.5319904827640713,\n",
              "  0.5243868476443498,\n",
              "  0.5226072272702343,\n",
              "  0.5196603328040225,\n",
              "  0.5182684990337214,\n",
              "  0.5107123131320972,\n",
              "  0.5056067885886659,\n",
              "  0.5045438057257391,\n",
              "  0.49249137926942893,\n",
              "  0.4900593718054521],\n",
              " [0.15,\n",
              "  0.16,\n",
              "  0.17,\n",
              "  0.18000000000000002,\n",
              "  0.19000000000000003,\n",
              "  0.20000000000000004,\n",
              "  0.21000000000000005,\n",
              "  0.22000000000000006,\n",
              "  0.23000000000000007,\n",
              "  0.24000000000000007,\n",
              "  0.2500000000000001,\n",
              "  0.2600000000000001,\n",
              "  0.27000000000000013,\n",
              "  0.28000000000000014,\n",
              "  0.29000000000000015,\n",
              "  0.30000000000000016,\n",
              "  0.31000000000000016,\n",
              "  0.3200000000000002,\n",
              "  0.3300000000000002,\n",
              "  0.3400000000000002,\n",
              "  0.3500000000000002,\n",
              "  0.3600000000000002,\n",
              "  0.3700000000000002,\n",
              "  0.3800000000000002,\n",
              "  0.39000000000000024,\n",
              "  0.40000000000000024,\n",
              "  0.41000000000000025,\n",
              "  0.42000000000000026,\n",
              "  0.43000000000000027,\n",
              "  0.4400000000000003,\n",
              "  0.4500000000000003,\n",
              "  0.4600000000000003,\n",
              "  0.4700000000000003,\n",
              "  0.4800000000000003,\n",
              "  0.4900000000000003])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q-HF0F1iSw_"
      },
      "source": [
        "### Retraining on validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4_mhXTMiR0g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRhKPqmqh7Hm"
      },
      "source": [
        "### Predicting toxic span sequences on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D501km2V_wCs"
      },
      "source": [
        "X_test=clf.X_to_sequences(test_df.tokens.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2c4YIa2ci7w",
        "outputId": "7679dafa-fda1-4d5d-b71c-6037cea60c7e"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us2Y7KWwbFA3"
      },
      "source": [
        "preds=model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z67pA4U-bGcf"
      },
      "source": [
        "label_predictions=[]\n",
        "\n",
        "for i,p in enumerate(preds):\n",
        "  label_predictions.append(clf.tune_threshold(p,len(test_df.iloc[i].labels),th=0.23))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvIJEN8MbPY-"
      },
      "source": [
        "span_predictions=[]\n",
        "f1_scores=[]\n",
        "for i,row in test_df.iterrows():\n",
        "  curr_pred_span=[]\n",
        "\n",
        "  for j,pred_labl in enumerate(label_predictions[i]):\n",
        "    if(pred_labl==2):\n",
        "      curr_pred_span+=list(np.arange(row.offset_mapping[j][0],row.offset_mapping[j][1]))\n",
        "\n",
        "  f1_scores.append(f1(curr_pred_span,row.spans))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grtkgI2ebeLF",
        "outputId": "eaff6605-92dd-4d73-e793-52f5304db73d"
      },
      "source": [
        "np.mean(f1_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6305217774547157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5ittMJpr_U0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}